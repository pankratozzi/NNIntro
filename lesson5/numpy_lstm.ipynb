{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c5bd7c",
   "metadata": {},
   "source": [
    "**Реализация LSTM cell через numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a89ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74d8558e",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=lstm_images/l1.png width=\"500\"/>\n",
    "</div>\n",
    "Ячейки LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ceb968b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"lstm_images/p.png\" width=\"500\"/>\n",
    "</div>\n",
    "Обозначения"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb2097a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=lstm_images/l2.png width=\"500\"/>\n",
    "</div>\n",
    "Определенить, какую информацию можно выбросить из состояния ячейки"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c98cfde6",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=lstm_images/l3.png width=\"500\"/>\n",
    "</div>\n",
    "Решить, какая новая информация будет храниться в состоянии ячейки"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "617ec6a0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=lstm_images/l4.png width=\"500\"/>\n",
    "</div>\n",
    "Заменить старое состояние ячейки $C_{t-1}$ на новое состояние $C_t$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b5733d3",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=lstm_images/l5.png width=\"500\"/>\n",
    "</div>\n",
    "Решить, какую информацию мы хотим получать на выходе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12164f0b",
   "metadata": {},
   "source": [
    "**Решение задачи предсказания следующего слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a69df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательные функции: one-hot encoding, создание последовательностей для работы, создание словаря\n",
    "\n",
    "def generate_sequences(num_sequences=300, random=False):\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 10)\n",
    "        sample_main = ['a'] * num_tokens + ['b'] * num_tokens\n",
    "        if random:\n",
    "            np.random.shuffle(sample_main)\n",
    "        sample = sample_main + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "sequences = generate_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fea5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_dicts(sequences):\n",
    "    # построение одного списка из всех последовательностей\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    all_words = flatten(sequences)\n",
    "    # Подсчет уникальных токенов в последовательностях\n",
    "    word_count = {}\n",
    "    for word in all_words:\n",
    "        if word in word_count.keys():\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    # Сортируем по частоте\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "    # Список всех уникальных токенов\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    # Добавим UNK токен в список уникальных: нужно если встретим новое слово\n",
    "    unique_words.append('UNK')\n",
    "    # количество последовательностей и уникальных слов\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Построим словари\n",
    "    word_to_idx = {}\n",
    "    idx_to_word = {}\n",
    "    \n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4629fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(sequences, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # количество последовательностей в каждом поднаборе\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # разделим список последовательностей на поднаборы\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        inputs, targets = [], []\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])    \n",
    "        return inputs, targets\n",
    "    \n",
    "    def get_dataset_generator(inputs, targets):\n",
    "        for inp, tar in zip(inputs, targets):\n",
    "            yield (inp, tar)\n",
    "\n",
    "    # Получаем фичи и таргеты для каждого поднабора\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # создаем генераторы для каждого поднабора\n",
    "    training_set = get_dataset_generator(inputs_train, targets_train)\n",
    "    validation_set = get_dataset_generator(inputs_val, targets_val)\n",
    "    test_set = get_dataset_generator(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3160b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set, test_set = create_datasets(sequences)\n",
    "training_set, validation_set, test_set = list(training_set), list(validation_set), list(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb27eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    # представляем каждое слово как one-hot вектор\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[idx] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    # преобразуем каждое слово в последовательности, получаем [num_sequences, vocab_size, 1]\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e43762",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50  # размерность латентного пространства\n",
    "vocab_size  = len(word_to_idx)  # размер словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7553949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_orthogonal(param):\n",
    "    # инициализация весов: берем параметры (https://arxiv.org/pdf/math-ph/0609050.pdf)\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    new_param = q\n",
    "    \n",
    "    return new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461db4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    return f\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    x_s = sigmoid(x)\n",
    "    return x_s * (1 - x_s)\n",
    "\n",
    "def tanh(x):\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    return f\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1 - tanh(x)**2\n",
    "\n",
    "def softmax(x):\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386296d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сумма размера латентного вектора и входного вектора - потребуется при конкатенации ht-1 и xt\n",
    "z_size = hidden_size + vocab_size \n",
    "\n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "    # инициализация всех весовых матриц в слое LSTM\n",
    "    # forget gate\n",
    "    W_f = np.random.randn(hidden_size, z_size)\n",
    "    b_f = np.zeros((hidden_size, 1))  # bias\n",
    "\n",
    "    # input gate\n",
    "    W_i = np.random.randn(hidden_size, z_size)\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # candidate - Ct_hat\n",
    "    W_c = np.random.randn(hidden_size, z_size)\n",
    "    b_c = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # output gate\n",
    "    W_o = np.random.randn(hidden_size, z_size)\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # веса для выходного \"слоя\"\n",
    "    W_v = np.random.randn(vocab_size, hidden_size)\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_c = init_orthogonal(W_c)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_c, W_o, W_v, b_f, b_i, b_c, b_o, b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "045f82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffb84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    inputs -- последовательность X(t): (len_seq, m).\n",
    "    h_prev -- Hidden state h(t-1):  (hidden_size, m)\n",
    "    C_prev -- Состояние ячейки C(t-1): (hidden_size, m)\n",
    "    p - список обучаемых параметров для каждого шага LSTM ячейки\n",
    "    На выходе:\n",
    "    z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s - список вычислений на каждом этапе (нужно для backward)\n",
    "    outputs - предсказания на шаге t: (n_v, m), здесь к LSTM добавили линейный слой для вычисления итоговых предсказаний\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "\n",
    "    W_f, W_i, W_c, W_o, W_v, b_f, b_i, b_c, b_o, b_v = p\n",
    "    \n",
    "    # списки для хранения результатов промежуточных вычислений\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_c, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # храним начальные состояния ячейки и латентного \"слоя\"\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    # идем ровно по схеме LSTM для каждой \n",
    "    for x in inputs:\n",
    "        # объединяем вход и hidden_state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "\n",
    "        # вычисляем forget gate как указано в формуле на схеме\n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # вычисляем input gate\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # вычисляем candidate (С(t)_hat)\n",
    "        c = tanh(np.dot(W_c, z) + b_c)\n",
    "        g_c.append(c)\n",
    "        \n",
    "        # вычисляем новое состояние ячейки (C(t))\n",
    "        C_prev = f * C_prev + i * c \n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # вычисляем выход из ячейки LSTM\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # вычисляем h(t-1) - hidden_state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # вычисляем выход сети logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        \n",
    "        # применяем softmax\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "\n",
    "    return z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s, output_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40eb8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    подрезаем градиенты так, чтобы норма вектора весов не превышала заданный порог\n",
    "    Таким образом мы предотвращаем \"взрыв\" градиентов\n",
    "    \"\"\" \n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # вычисляем квадрат L2 нормы каждого градиента и аккумулируем их в total_norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # определяем коэффициент для подрезки\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # если норма градиентов превышает допустимую, то подрезаем каждый градиент\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def backward(z, f, i, c, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Подаем вычисления с forward pass и текущие значения весов во всех частях LSTM-cell\n",
    "    Получаем значение функции потерь и значения вычисленных на каждом шаге градиентов\n",
    "    \"\"\"\n",
    "    W_f, W_i, W_c, W_o, W_v, b_f, b_i, b_c, b_o, b_v = p\n",
    "\n",
    "    # Зануляем градиенты (по аналогии с PyTorch: optimizer.zero_grad())\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_c_d = np.zeros_like(W_c)\n",
    "    b_c_d = np.zeros_like(b_c)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Устанавливаем следующие значения ячейки и hidden_state в 0\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):  # идем в обратном направлении (по аналогии: от вершин графа к началу)\n",
    "        \n",
    "        # вычисляем cross entropy\n",
    "        loss -= np.mean(np.log(outputs[t]) * targets[t])\n",
    "        # берем предыдущее значение ячейки\n",
    "        C_prev= C[t-1]\n",
    "        \n",
    "        # вычисляем производную выхода по hidden_state и обновляем соответствующие веса\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # вычисляем производную hidden state и выхода, обновляем веса на выходе\n",
    "        dh = np.dot(W_v.T, dv)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid_deriv(o[t]) * do\n",
    "        \n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # вычисляем производную C(t) и кандидата C(t)_hat и обновляем соответствующие веса\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh_deriv(tanh(C[t]))\n",
    "        dc = dC * i[t]\n",
    "        dc = tanh_deriv(c[t]) * dc\n",
    "        \n",
    "        W_c_d += np.dot(dc, z[t].T)\n",
    "        b_c_d += dc\n",
    "\n",
    "        # вычисляем производную input gate и обновляем соответствующие веса\n",
    "        di = dC * c[t]\n",
    "        di = sigmoid_deriv(i[t]) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # вычисляем производную forget gate и обновляем соответствующие веса\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # вычисляем производную на входе и обновляем веса C(t-1), h(t-1)\n",
    "        dz = (np.dot(W_f.T, df) + np.dot(W_i.T, di) + np.dot(W_c.T, dc) + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hidden_size, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads = W_f_d, W_i_d, W_c_d, W_o_d, W_v_d, b_f_d, b_i_d, b_c_d, b_o_d, b_v_d\n",
    "    \n",
    "    # подрезаем градиенты\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c85e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # функция для обновления параметров сети\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d68c78",
   "metadata": {},
   "source": [
    "**Соберем все вместе**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68311d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 2.141402711587014, validation loss: 3.2885032998166666\n",
      "Epoch 10, training loss: 0.8373650800754653, validation loss: 0.815229374216115\n",
      "Epoch 20, training loss: 0.6991599610368799, validation loss: 0.7285838213752872\n",
      "Epoch 30, training loss: 0.7058006688789625, validation loss: 0.7118651832025098\n",
      "Epoch 40, training loss: 0.74876888490035, validation loss: 0.6689255619627217\n",
      "Epoch 50, training loss: 0.7652350093654167, validation loss: 0.6689943238596222\n",
      "Epoch 60, training loss: 0.7731757524748584, validation loss: 0.6994437462185611\n",
      "Epoch 70, training loss: 0.7273154729603952, validation loss: 0.6877123683680036\n",
      "Epoch 80, training loss: 0.6856806411340195, validation loss: 0.6563393902498961\n",
      "Epoch 90, training loss: 0.6518313661143267, validation loss: 0.6351913630632194\n",
      "Epoch 100, training loss: 0.6447907310155416, validation loss: 0.6300643348625169\n",
      "Epoch 110, training loss: 0.6440208689522087, validation loss: 0.6286942588785318\n",
      "Epoch 120, training loss: 0.6456730033764713, validation loss: 0.629886510782648\n",
      "Epoch 130, training loss: 0.6474625316238023, validation loss: 0.6316791482942202\n",
      "Epoch 140, training loss: 0.6458655976823717, validation loss: 0.6305650904885168\n",
      "Epoch 150, training loss: 0.6439370802991002, validation loss: 0.6287500001322072\n",
      "Epoch 160, training loss: 0.64334285483031, validation loss: 0.6281878945972639\n",
      "Epoch 170, training loss: 0.6430099959626132, validation loss: 0.6279098412034677\n",
      "Epoch 180, training loss: 0.6417247925954561, validation loss: 0.626844630626837\n",
      "Epoch 190, training loss: 0.6383268377730639, validation loss: 0.6242021321095247\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'EOS']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlN0lEQVR4nO3deXwV9b3/8dcnJxuQAEKiyCaggIpAAgEXBEG9LagVtVihXpHa4vKzdWtr1d4Kt8vjcdtyWy916aW1Ur1a9NbKxQriLihVDPuqIsYaZQlhCyQsSb6/P2ZOcsiekMlJmPfz8ZjHmTNnzjmfTJJ5n+93Zr7HnHOIiEh4JcS7ABERiS8FgYhIyCkIRERCTkEgIhJyCgIRkZBLjHcBjZWRkeH69OkT7zJERNqUFStW7HLOZdb0WJsLgj59+pCbmxvvMkRE2hQz+6y2x9Q1JCIScgoCEZGQUxCIiIRcmztGICIt7+jRo+Tn53Po0KF4lyL1SE1NpWfPniQlJTX4OQoCEalXfn4+6enp9OnTBzOLdzlSC+cchYWF5Ofn07dv3wY/T11DIlKvQ4cO0bVrV4VAK2dmdO3atdEtNwWBiDSIQqBtaMrvKTRBsH49/OQnsHNnvCsREWldQhMEmzbBz3+uIBBpiwoLC8nKyiIrK4tu3brRo0ePivtHjhyp87m5ubnccccd9b7HBRdc0Cy1vvXWW1xxxRXN8lotJTQHiyMR77asLL51iEjjde3aldWrVwMwc+ZM0tLS+MEPflDxeGlpKYmJNe/OcnJyyMnJqfc9li1b1iy1tkWhaREoCEROLNOmTePWW2/l3HPP5d5772X58uWcf/75ZGdnc8EFF/Dhhx8Cx35CnzlzJjfddBNjx46lX79+zJ49u+L10tLSKtYfO3YskyZN4swzz+T6668n+k2OCxcu5Mwzz2T48OHccccd9X7y3717N1dddRVDhgzhvPPOY+3atQC8/fbbFS2a7OxsioqK2LZtG2PGjCErK4tzzjmHpUuXNvs2q41aBCLSOHfdBf6n82aTlQUPPdTop+Xn57Ns2TIikQj79+9n6dKlJCYm8tprr/HAAw/w/PPPV3vO5s2befPNNykqKmLgwIHcdttt1c65X7VqFRs2bKB79+6MGjWKd999l5ycHG655RaWLFlC3759mTJlSr31zZgxg+zsbObPn88bb7zB1KlTWb16NbNmzeKRRx5h1KhRHDhwgNTUVObMmcNXv/pVfvzjH1NWVkZxcXGjt0dTKQhEpM269tprifj/3Pv27ePGG2/k448/xsw4evRojc+5/PLLSUlJISUlhZNPPpkdO3bQs2fPY9YZOXJkxbKsrCzy8vJIS0ujX79+FefnT5kyhTlz5tRZ3zvvvFMRRhdffDGFhYXs37+fUaNGcc8993D99ddzzTXX0LNnT0aMGMFNN93E0aNHueqqq8jKyjqeTdMoCgIRaZwmfHIPSocOHSrmf/KTnzBu3DheeOEF8vLyGDt2bI3PSUlJqZiPRCKUlpY2aZ3jcd9993H55ZezcOFCRo0axeLFixkzZgxLlizhpZdeYtq0adxzzz1MnTq1Wd+3NjpGICInhH379tGjRw8A5s6d2+yvP3DgQLZu3UpeXh4Azz77bL3PGT16NE8//TTgHXvIyMigY8eOfPLJJwwePJgf/ehHjBgxgs2bN/PZZ59xyimnMH36dL7zne+wcuXKZv8ZaqMgEJETwr333sv9999PdnZ2s3+CB2jXrh2PPvoo48ePZ/jw4aSnp9OpU6c6nzNz5kxWrFjBkCFDuO+++/jzn/8MwEMPPcQ555zDkCFDSEpKYsKECbz11lsMHTqU7Oxsnn32We68885m/xlqY9Gj4W1FTk6Oa8oX0yxdCmPGwKuvwqWXBlCYyAls06ZNnHXWWfEuI+4OHDhAWloazjluv/12+vfvz9133x3vsqqp6fdlZiucczWeR6sWgYhIA/3hD38gKyuLQYMGsW/fPm655ZZ4l9QsQnOwOMGPvPLy+NYhIm3X3Xff3SpbAMdLLQIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQkVZv3LhxLF68+JhlDz30ELfddlutzxk7dizRU80vu+wy9u7dW22dmTNnMmvWrDrfe/78+WzcuLHi/oMPPshrr73WiOpr1pqGq1YQiEirN2XKFObNm3fMsnnz5jVo4DfwRg3t3Llzk967ahD89Kc/5dIT7GIkBYGItHqTJk3ipZdeqvgSmry8PL788ktGjx7NbbfdRk5ODoMGDWLGjBk1Pr9Pnz7s2rULgF/84hcMGDCACy+8sGKoavCuERgxYgRDhw7l61//OsXFxSxbtowFCxbwwx/+kKysLD755BOmTZvGX//6VwBef/11srOzGTx4MDfddBOHDx+ueL8ZM2YwbNgwBg8ezObNm+v8+eI9XHVg1xGYWSqwBEjx3+evzrkZVdZJAZ4EhgOFwHXOubwg6lEQiDSPeIxC3aVLF0aOHMmiRYuYOHEi8+bN4xvf+AZmxi9+8Qu6dOlCWVkZl1xyCWvXrmXIkCE1vs6KFSuYN28eq1evprS0lGHDhjF8+HAArrnmGqZPnw7Av/3bv/H444/zve99jyuvvJIrrriCSZMmHfNahw4dYtq0abz++usMGDCAqVOn8thjj3HXXXcBkJGRwcqVK3n00UeZNWsWf/zjH2v9+eI9XHWQLYLDwMXOuaFAFjDezM6rss63gT3OuTOA3wK/DKoYBYFI2xbbPRTbLfTcc88xbNgwsrOz2bBhwzHdOFUtXbqUq6++mvbt29OxY0euvPLKisfWr1/P6NGjGTx4ME8//TQbNmyos54PP/yQvn37MmDAAABuvPFGlixZUvH4NddcA8Dw4cMrBqqrzTvvvMMNN9wA1Dxc9ezZs9m7dy+JiYmMGDGCJ554gpkzZ7Ju3TrS09PrfO2GCKxF4LxBjA74d5P8qerARhOBmf78X4GHzcxcAAMgKQhEmke8RqGeOHEid999NytXrqS4uJjhw4fz6aefMmvWLD744ANOOukkpk2bxqFDh5r0+tOmTWP+/PkMHTqUuXPn8tZbbx1XvdGhrI9nGOuWGq460GMEZhYxs9XATuBV59z7VVbpAXwO4JwrBfYBXYOoRUEg0ralpaUxbtw4brrpporWwP79++nQoQOdOnVix44dLFq0qM7XGDNmDPPnz6ekpISioiJefPHFiseKioo49dRTOXr0aMXQ0QDp6ekUFRVVe62BAweSl5fHli1bAHjqqae46KKLmvSzxXu46kDHGnLOlQFZZtYZeMHMznHOrW/s65jZzcDNAL17925SLQoCkbZvypQpXH311RVdRNFhm88880x69erFqFGj6nz+sGHDuO666xg6dCgnn3wyI0aMqHjsZz/7Geeeey6ZmZmce+65FTv/yZMnM336dGbPnl1xkBggNTWVJ554gmuvvZbS0lJGjBjBrbfe2qSfK/pdykOGDKF9+/bHDFf95ptvkpCQwKBBg5gwYQLz5s3j17/+NUlJSaSlpfHkk0826T1jtdgw1Gb2IFDsnJsVs2wxMNM59w8zSwS2A5l1dQ01dRjqPXugSxf47W+9g10i0nAahrptaTXDUJtZpt8SwMzaAf8CVD2HagFwoz8/CXgjiOMDoBaBiEhtguwaOhX4s5lF8ALnOefc383sp0Cuc24B8DjwlJltAXYDk4MqRkEgIlKzIM8aWgtk17D8wZj5Q8C1QdUQS0Egcnycc5hZvMuQejSlU0VXFotIvVJTUyksLGzSTkZajnOOwsJCUlNTG/W80HxDmYJApOl69uxJfn4+BQUF8S5F6pGamkrPnj0b9ZzQBEG0RasgEGm8pKQk+vbtG+8yJCCh6Roy8763WN9ZLCJyrNAEAXjdQ2oRiIgcS0EgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnKhCoKEBAWBiEhVoQqCSETfWSwiUlXogkAtAhGRYykIRERCTkEgIhJyCgIRkZBTEIiIhFxgQWBmvczsTTPbaGYbzOzOGtYZa2b7zGy1Pz0YVD2gIBARqUligK9dCnzfObfSzNKBFWb2qnNuY5X1ljrnrgiwjgoKAhGR6gJrETjntjnnVvrzRcAmoEdQ79cQCgIRkepa5BiBmfUBsoH3a3j4fDNbY2aLzGxQLc+/2cxyzSy3oKCgyXUoCEREqgs8CMwsDXgeuMs5t7/KwyuB05xzQ4HfAfNreg3n3BznXI5zLiczM7PJtSgIRESqCzQIzCwJLwSeds79rerjzrn9zrkD/vxCIMnMMoKqR0EgIlJdkGcNGfA4sMk595ta1unmr4eZjfTrKQyqJgWBiEh1QZ41NAq4AVhnZqv9ZQ8AvQGcc78HJgG3mVkpUAJMds65oApSEIiIVBdYEDjn3gGsnnUeBh4OqoaqFAQiItXpymIRkZBTEIiIhJyCQEQk5EIVBPqqShGR6kIVBGoRiIhUF7og0HcWi4gcK3RBoBaBiMixQhcE5eUQ3CVrIiJtT+iCANQ9JCISK5RBoO4hEZFKCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQC08QfPQRkVdfBhQEIiKxwhMEa9YQeeZJQEEgIhIrPEGQkkIELwEUBCIilRQEIiIhF6ogSMAbf1pBICJSKVRBoBaBiEh1oQwCfTGNiEilBgWBmXUwswR/foCZXWlmScGW1sySk9UiEBGpQUNbBEuAVDPrAbwC3ADMDaqoQKhrSESkRg0NAnPOFQPXAI86564FBtX5BLNeZvammW00sw1mdmcN65iZzTazLWa21syGNf5HaCAFgYhIjRocBGZ2PnA98JK/LFLPc0qB7zvnzgbOA243s7OrrDMB6O9PNwOPNbCexlMQiIjUqKFBcBdwP/CCc26DmfUD3qzrCc65bc65lf58EbAJ6FFltYnAk87zHtDZzE5tzA/QYAoCEZEaJTZkJefc28DbAP5B413OuTsa+iZm1gfIBt6v8lAP4POY+/n+sm1Vnn8zXouB3r17N/Rtj6UgEBGpUUPPGnrGzDqaWQdgPbDRzH7YwOemAc8Ddznn9jelSOfcHOdcjnMuJzMzsykvobOGRERq0dCuobP9nfhVwCKgL96ZQ3XyTzF9HnjaOfe3Glb5AugVc7+nv6z5RSJE/J9WQSAiUqmhQZDk79SvAhY4544Crq4nmJkBjwObnHO/qWW1BcBU/+yh84B9zrlttax73CLJ3vFtBYGISKUGHSMA/hvIA9YAS8zsNKC+bp5ReK2GdWa22l/2ANAbwDn3e2AhcBmwBSgGvtWI2hstkpQAhxQEIiKxGnqweDYwO2bRZ2Y2rp7nvANYPes44PaG1NAc1CIQEamuoQeLO5nZb8ws15/+E+gQcG3NLpLi5Z6CQESkUkOPEfwJKAK+4U/7gSeCKiookSTvx1UQiIhUaugxgtOdc1+Puf/vMf3+bYZaBCIi1TW0RVBiZhdG75jZKKAkmJKCo2MEIiLVNbRFcCvwpJl18u/vAW4MpqTgqEUgIlJdQ88aWgMMNbOO/v39ZnYXsDbA2pqdgkBEpLpGfUOZc25/zDAR9wRQT6ASUrzv0lEQiIhUOp6vqqzzGoHWSMcIRESqO54gqHOIidYokuq1CPSdxSIileo8RmBmRdS8wzegXSAVBUjHCEREqqszCJxz6S1VSEuItggUBCIilY6na6jNURCIiFSnIBARCTkFgYhIyIUqCBJSkwEoK21zJzyJiAQmVEHgfYF9KWVH1CQQEYkKYRCUKQhERGI0dNC5E0M0CI7GuxARkdYjnEFwpM2NjiEiEphwBUFysoJARKSKcAVBRdeQgkBEJCqkB4s16pyISFRIWwTxLkREpPUIaRCoa0hEJCqcXUNH1TUkIhIVriBITiaBcspKFQQiIlHhCoKKFoHGGhIRiQpnEKhFICJSIbAgMLM/mdlOM1tfy+NjzWyfma32pweDqqWCHwTlGn1URKRCkGcNzQUeBp6sY52lzrkrAqzhWCkpRDigYahFRGIE1iJwzi0Bdgf1+k1S0TWkIBARiYr3MYLzzWyNmS0ys0G1rWRmN5tZrpnlFhQUNP3dkpNpTzEHDkWa/hoiIieYeAbBSuA059xQ4HfA/NpWdM7Ncc7lOOdyMjMzm/6OKSlksIvCA6lNfw0RkRNM3ILAObffOXfAn18IJJlZRqBvmpREJgXsOqggEBGJilsQmFk3MzN/fqRfS2HAb0pGZA+7itvjdJhARAQI8KwhM/sLMBbIMLN8YAaQBOCc+z0wCbjNzEqBEmCyc8HvnjMS93H4cBIHD0JaWtDvJiLS+gUWBM65KfU8/jDe6aUtKiN5HxyGXbsUBCIiEP+zhlpcRnIR4AWBiIiEMQhSFAQiIrHCFwTtDgJwPJcjiIicSMIXBO2LAbUIRESiQhcEnft0JkKpgkBExBe6ILAzTieDQnYV6EICEREIYRBwxhlkUMCuLw7FuxIRkVYhpEGwi11fHI53JSIirUJ4g0BdQyIiQBiDoHdvMmw3u/YmxbsSEZFWIXxBkJhIRudSCkvaUa6vLhYRCWEQAJmnJFDmIuzbF+9KRETiL5RBkNGrHQAFO3WcQEQklEFwyhnpAHy+Xk0CEZFQBsGIcWlEKOWt/9sb71JEROIulEHQ6avnMdI+4LXXLN6liIjEXSiDgI4dubTfp3ywrQf79uo4gYiEWziDALh0YgfKSOTt//k83qWIiMRVaIPgvO+NoD0Hee2ZHfEuRUQkrkIbBMl9ujO202oWreqGU++QiIRYaIMA4IrzdrHlUC8+XFUc71JEROIm3EHwr50BePGx/PgWIiISR6EOgl4Th5HFKl5cnBzvUkRE4ibUQUB6Ol/rvpJ3P+9FYWG8ixERiY9wBwFw+bhiyonwxsv6ohoRCafQB0H2pNNJ5jDLF+g0UhEJp9AHQfJF55PNKj5YrnNIRSScQh8EnHQSIzpvIffzkykri3cxIiItL7AgMLM/mdlOM1tfy+NmZrPNbIuZrTWzYUHVUp+Rg0s4WNaOzZvUKhCR8AmyRTAXGF/H4xOA/v50M/BYgLXUacSlnQBY/ved8SpBRCRuAgsC59wSYHcdq0wEnnSe94DOZnZqUPXUZcBVZ9ORfSx/ZW883l5EJK7ieYygBxA79Ge+v6waM7vZzHLNLLegoKDZC0kYdBYjIit5b237Zn9tEZHWrk0cLHbOzXHO5TjncjIzM5v/DSIRLu77KasLe7HjSx0xFpFwiWcQfAH0irnf018WF+On9wbglZ8vj1cJIiJxEc8gWABM9c8eOg/Y55zbFq9isu65mJMjhbz89C50HqmIhEmQp4/+BfgHMNDM8s3s22Z2q5nd6q+yENgKbAH+APy/oGppiITEBMaPKmLx/vMp++6dcORIPMsREWkxiUG9sHNuSj2PO+D2oN6/Kcbf3JsnlyTw7u/XMmb9JbBwIaSnx7ssEZFAtYmDxS1lwuUJdOsG13VezMfLCuDyy+HgwXiXJSISKAVBjM6d4fXXoSypHaPTV/HWO4nw3e/GuywRkUApCKo4+2x4+23o3K0dl/Aaz8/dD3/7W7zLEhEJjIKgBmedBR98ACNGwLcjc8n79s8ggAvZRERaAwVBLdLT4Zm/JFCe2p4b9v2O8jvvjndJIiKBUBDUoV8/+O1/RXjHXcgzf8E7i0hE5ASjIKjHt74FOcPL+VHif3Lg5nugqCjeJYmINCsFQT0SEuC/ZifwZekp/PKLf4UHHoh3SSIizUpB0AAXXADf/CbMitxL3sMvwmuvxbskEZFmoyBooP/4D7DkJO5NfwymTtVZRFU5ByUlsH+/NzyH07e9ibQVgQ0xcaLp1Qvuu8+YMWMCb5eczUXXXQcvvwzJyfEurXkcPgz79sHevd5t1akhy48erXy9hATo1An69PGmAQPgwgu9qXPnePyEIlILc23sk1tOTo7Lzc2Ny3sXF3vXGHSx3eR+lknkhuth7lxvp9calZfDP/8JmzbBp5/C9u2wY4d3u3077NlTuRM/fLj+10tP93bunTp5O/PofOyUnAyHDnlTYSF89pn33lu2eEFhBllZcOml8JWveMGQmhr0lhAJPTNb4ZzLqekxtQgaoX17+NWvYPLkLjx6+Ut876kJXnfIU081aGe2bRusXAl5ebB7t7dP7NTJO001OqWkHEeB27fDsmXe9O67sGaNV1+UGWRmQrducMop0Ldv/Tv26PL0dIhEml5bSQm8/z4sWQJvvAEPPQS//rW33UaP9kLh0kth0CBISjqOjSAijaUWQSM5B1dcAa+84njjlucY/chkb+f13/8No0ZVW3/3bnjuOS8rli2r+7WTk2HkSBgzBi66yLutNV+cg82b4Z13KqetW73HUlK8y6JzcrwxM848E04/HU4+GRJbSfYfOOCFwiuvwKuvwsaN3vKUFBg8GLKzK6fBg6FDh/jWK9LG1dUiUBA0wd69cO65Xs/HM3d/wFfmTPK6YEaPhmnTODLuqyxc3Z2n/sf4+9+9Y6eDBsH113s9If37Q9eu3mvt3u3tvz/5xPsAv3QprFgBpaXQrh2MHQvjLy1l/Okf03//Cmz9Oli3DpYv9woA71P+hRd6QTRqlLfzPK6mRRzk53uDPK1aVTnt2eM9ZuYdY8jK8qacHG/SsQaRBlMQBGDLFrjqKtiwAb42oZTRCe9Svuw9Vu05jVf5F3bTlZOTdvPN/rlMHbmZrDMPYV1O8j72JyZ63R+JiV4/fnFx5XTwIAfz97Bk3Uks3tqfRYUj+Kj0dAD6spUJCYsZ33sT40YdIe3ikZXJYhbfDeIrL/cOCaxe7U3r1nldYkVFXs9SRgb07OkdfO/Tp7JLrFevKj1CznnhGn2h6JSXV7nOwIFeE2rkSC+ZhwxpewEo0kIUBAEpKYF//3eYN887JgrQPfMol/TZwuSM1/nKwRdI3LIZdu1q3DeenXQSdO8OPXpA9+5sTRvC4pIxvLx1AK8vT+PgQSMpyWuATJjgdSGdc453DKOlOOf9WB9/7IXh6tVei2bNGq/XB7wd/4AB3k4+Pd37BtCCAu/D/xdfeK2eqEjEC4hTT/Wmbt28KXa+Wzc4JXkPyWtzvRbR8uXecYcdO7wXSU72WgzRcBg50gvJ1nowX6QFKQgC5pzXi5GS4u2Mq304j55jv2ePFwilpd4ZNNE9YYcO3hM7dPD6g+o4WHr4sHcc+OWXYdEiWL/eW56Q4H1APussOO00b+revfqx36Qkb92q05Ej3ok+JSWVJ/1ES96509uB79zpfbrfsgU++sjrIotKT4ehQyt7b4YO9brD2rWr+ecoK4Mvv/S6xaJT9MSm6BTt+Ypl5rUgBg/2p3McgzO3c0bBP0hc8b4XDrm5lWnUqZN3vCQaDNnZXjK1khaUSEtREJzA8vO9IbOjPScffeS1TmJPFmou7dp5n8pPP937pN+/vzeddZbXzdPcH7yPHDn2bNft272fd+NGr8vp44+9rijwQvjss/1wGFTO4M7/ZPCB9zj1o7exD5bD2rWVwZue7qXUoEHek3r18lKze3evCaLTWeUEpCAImWi3zfbtx177tX+/ty8sL68+JSV5O/rU1Mrb1FSvl+rkk73j0a3txJ2SEu8SiXX+8fO1a73b7dsr1+naFc44A3r3KKV3yk56lX5K7wMb6V2QS7et/6Dr7o9Ipco1FOnp3tSx47HzHTp43U9JSd5tTfNJSV5rIyHBu6061bbc7NhfSFlZzb+oxk7OVd7WNl/b49F6a5vqe7y5prb+Pq2k9akgkFDZtasyHNat87qcPv/cO/Z86FD19dulltM17TBdUkvoYMW05yDtKKG9K6Zd+UHalxXRvqyIlNKDJJUfJrHsMInlh0ksO0Ji2WGSykpIpLTGKYmjJOA1WwxXcduYecC/ZxXz1W4tAWfejseZt/NxloBL8K79iF1W4y2GS0jw3tkSKl/XOZxzUO68UUPKy71b53Dlzqsw+pgfJOa8KcGVYjgSKG/wbX3rtOYpWn8C5UQoO2beagqbSKTm+bruT58O3/9+k/4vdEGZhEpGBowb502xoi2lf/7Tm3bu9I5DFBYmsHt3O3bvbkdxcReKi2F3iXcSV0kJFB/xbktKKruiWh3nT621vrArhwRXTsTKSSh3JJgjUlru3Zp3m4AjkuCHiTkilJNg5d5yKyMBx/RVO7gngPIUBBIa0QurMzNh+PCmvYZzXq9Naemxx/xrmo4e9aZoT0v0+Y2dj/YsRHuQovMNvQ1q3dqeU1OvU2Nu61untU7R+mJ79aLz3m0C5eUJNT7e0Plul/Wo+w+0iRQEIo1g5l3+0Vou0BZpDjrBWkQk5BQEIiIhpyAQEQk5BYGISMgFGgRmNt7MPjSzLWZ2Xw2PTzOzAjNb7U/fCbIeERGpLrBzH8wsAjwC/AuQD3xgZguccxurrPqsc+67QdUhIiJ1C7JFMBLY4pzb6pw7AswDJgb4fiIi0gRBBkEP4POY+/n+sqq+bmZrzeyvZtarphcys5vNLNfMcgsKCoKoVUQktOJ9WcyLwF+cc4fN7Bbgz8DFVVdyzs0B5gD4xxQ+a+L7ZQC7mlpswFprbaqrcVprXdB6a1NdjdPUuk6r7YEgg+ALIPYTfk9/WQXnXOyI838EflXfizrnMptakJnl1jboUry11tpUV+O01rqg9damuhoniLqC7Br6AOhvZn3NLBmYDCyIXcHMTo25eyWwKcB6RESkBoG1CJxzpWb2XWAxEAH+5JzbYGY/BXKdcwuAO8zsSqAU2A1MC6oeERGpWaDHCJxzC4GFVZY9GDN/P3B/kDVUMacF36uxWmttqqtxWmtd0HprU12N0+x1tbkvphERkealISZEREJOQSAiEnKhCYL6xj1qwTp6mdmbZrbRzDaY2Z3+8plm9kXMuEuXxaG2PDNb579/rr+si5m9amYf+7cnxaGugTHbZbWZ7Tezu+KxzczsT2a208zWxyyrcRuZZ7b/N7fWzIa1cF2/NrPN/nu/YGad/eV9zKwkZrv9voXrqvX3Zmb3+9vrQzP7alB11VHbszF15ZnZan95S26z2vYRwf2dOf/LqU/kCe+spU+AfkAysAY4O061nAoM8+fTgY+As4GZwA/ivJ3ygIwqy34F3OfP3wf8shX8LrfjXRzT4tsMGAMMA9bXt42Ay4BFgAHnAe+3cF1fARL9+V/G1NUndr04bK8af2/+/8EaIAXo6//PRlqytiqP/yfwYBy2WW37iMD+zsLSImg14x4557Y551b680V4104E80WkzWMi3hXf+LdXxa8UAC4BPnHONfXq8uPinFuCd6pzrNq20UTgSed5D+hc5dqZQOtyzr3inCv1776Hd1Fni6ple9VmIjDPOXfYOfcpsAXvf7fFazMzA74B/CWo969NHfuIwP7OwhIEDR33qEWZWR8gG3jfX/Rdv2n3p3h0wQAOeMXMVpjZzf6yU5xz2/z57cApcagr1mSO/eeM9zaD2rdRa/q7uwnvU2NUXzNbZWZvm9noONRT0++tNW2v0cAO59zHMctafJtV2UcE9ncWliBodcwsDXgeuMs5tx94DDgdyAK24TVLW9qFzrlhwATgdjMbE/ug89qhcTvf2Lwr1K8E/tdf1Bq22THivY1qYmY/xrto82l/0Tagt3MuG7gHeMbMOrZgSa3u91aDKRz7gaPFt1kN+4gKzf13FpYgqHfco5ZkZkl4v+CnnXN/A3DO7XDOlTnnyoE/EGCTuDbOuS/8253AC34NO6LNTP92Z0vXFWMCsNI5twNaxzbz1baN4v53Z2bTgCuA6/2dB37XS6E/vwKvL35AS9VUx+8t7tsLwMwSgWuAZ6PLWnqb1bSPIMC/s7AEQb3jHrUUv+/xcWCTc+43Mctj+/SuBtZXfW7AdXUws/ToPN6BxvV42+lGf7Ubgf9rybqqOOZTWry3WYzattECYKp/Vsd5wL6Ypn3gzGw8cC9wpXOuOGZ5pnlfHIWZ9QP6A1tbsK7afm8LgMlmlmJmff26lrdUXTEuBTY75/KjC1pym9W2jyDIv7OWOAreGia8I+sf4SX5j+NYx4V4Tbq1wGp/ugx4CljnL18AnNrCdfXDO2NjDbAhuo2ArsDrwMfAa0CXOG23DkAh0ClmWYtvM7wg2gYcxeuL/XZt2wjvLI5H/L+5dUBOC9e1Ba/vOPp39nt/3a/7v+PVwErgay1cV62/N+DH/vb6EJjQ0r9Lf/lc4NYq67bkNqttHxHY35mGmBARCbmwdA2JiEgtFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgUoWZldmxo50222i1/iiW8breQaRGgX5VpUgbVeKcy4p3ESItRS0CkQbyx6f/lXnf2bDczM7wl/cxszf8QdReN7Pe/vJTzPsegDX+dIH/UhEz+4M/1vwrZtYubj+UCAoCkZq0q9I1dF3MY/ucc4OBh4GH/GW/A/7snBuCN7DbbH/5bOBt59xQvHHvN/jL+wOPOOcGAXvxrloViRtdWSxShZkdcM6l1bA8D7jYObfVHxRsu3Ouq5ntwhsm4ai/fJtzLsPMCoCezrnDMa/RB3jVOdffv/8jIMk59/MW+NFEaqQWgUjjuFrmG+NwzHwZOlYncaYgEGmc62Ju/+HPL8Mb0RbgemCpP/86cBuAmUXMrFNLFSnSGPokIlJdO/O/tNz3snMuegrpSWa2Fu9T/RR/2feAJ8zsh0AB8C1/+Z3AHDP7Nt4n/9vwRrsUaVV0jECkgfxjBDnOuV3xrkWkOalrSEQk5NQiEBEJObUIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5P4/xcwaXXEp8rsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "# инициализируем наши слои, получив начальные веса\n",
    "z_size = hidden_size + vocab_size\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "\n",
    "# Для LSTM инициализируем начальные значения hidden_state (надо с чего-то начинать)\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# будем сохранять значения потерь\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    # для каждой последовательности в проверочном наборе\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # кодируем последовательности через One-hot\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # инициализируем h_prev, C_prev нулями на старте\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Прямой проход\n",
    "        z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Обратный проход\n",
    "        loss, _ = backward(z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        # аккумулируем потери\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # Теперь тренировочный набор\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        loss, grads = backward(z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    training_loss.append(epoch_training_loss/len(list(training_set)))\n",
    "    validation_loss.append(epoch_validation_loss/len(list(validation_set)))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "\n",
    "    \n",
    "# берем первую пару из тестового набора\n",
    "inputs, targets = next(iter(test_set))\n",
    "\n",
    "# кодируем их\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# снова объявляем начальные значения C(t-1), h(t-1)\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Прогоняем через нашу сеть\n",
    "z_s, f_s, i_s, g_c, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# График функции потерь\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9721af",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "\n",
    "* https://habr.com/ru/company/wunderfund/blog/331310/\n",
    "* https://github.com/CaptainE/RNN-LSTM-in-numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
